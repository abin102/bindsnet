{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label (All Activity): 5\n",
      "Predicted Label (Proportion Weighting): 5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from bindsnet.models import DiehlAndCook2015\n",
    "from bindsnet.datasets import MNIST\n",
    "from bindsnet.encoding import PoissonEncoder\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from bindsnet.network.monitors import Monitor\n",
    "from bindsnet.evaluation import all_activity, assign_labels, proportion_weighting\n",
    "\n",
    "\n",
    "\n",
    "# Parameters (same as during training)\n",
    "n_neurons = 100  # Replace with the actual number of neurons used in training\n",
    "time = 250       # Replace with the actual time used during training\n",
    "dt = 1.0         # Replace with the actual time step\n",
    "intensity = 128  # Replace with the actual intensity\n",
    "n_classes = 10   # Number of classes (digits 0-9)\n",
    "\n",
    "# Set the device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on: {device}\")\n",
    "\n",
    "# Initialize the network architecture (same as during training)\n",
    "network = DiehlAndCook2015(\n",
    "    n_inpt=784,\n",
    "    n_neurons=n_neurons,\n",
    "    exc=22.5,    # Replace with actual excitatory current\n",
    "    inh=120,     # Replace with actual inhibitory current\n",
    "    dt=dt,\n",
    "    norm=78.4,   # Replace with actual normalization\n",
    "    theta_plus=0.05,  # Replace with actual threshold increment\n",
    "    inpt_shape=(1, 28, 28),  # Input shape for MNIST (channels, height, width)\n",
    ").to(device)\n",
    "\n",
    "# Load the saved weights, assignments, proportions, and rates\n",
    "checkpoint = torch.load('./snn_train.pt')\n",
    "\n",
    "network.load_state_dict(checkpoint['model_state_dict'])\n",
    "assignments = checkpoint['assignments'].to(device)\n",
    "proportions = checkpoint['proportions'].to(device)\n",
    "rates = checkpoint['rates'].to(device)\n",
    "\n",
    "# Set the network in evaluation mode\n",
    "network.train(mode=False)\n",
    "\n",
    "# Add a spike monitor to the 'Ae' layer\n",
    "spike_monitor = Monitor(network.layers[\"Ae\"], state_vars=[\"s\"], time=int(time / dt), device=device)\n",
    "network.add_monitor(spike_monitor, name=\"Ae_spikes\")\n",
    "\n",
    "# Load the MNIST test dataset\n",
    "test_dataset = MNIST(\n",
    "    PoissonEncoder(time=time, dt=dt),\n",
    "    None,\n",
    "    root=os.path.join(\"..\", \"..\", \"data\", \"MNIST\"),\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Lambda(lambda x: x * intensity)]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Get a single sample from the test dataset\n",
    "sample = test_dataset[1110]  # You can change the index for different samples\n",
    "\n",
    "# Prepare input for the network (move it to GPU if available)\n",
    "inputs = {\"X\": sample[\"encoded_image\"].view(int(time / dt), 1, 1, 28, 28).to(device)}\n",
    "\n",
    "# Run the network on the input\n",
    "network.run(inputs=inputs, time=time)\n",
    "\n",
    "# Record spikes from the excitatory layer\n",
    "spike_record = torch.zeros((1, int(time / dt), n_neurons), device=device)\n",
    "spike_record[0] = network.monitors[\"Ae_spikes\"].get(\"s\").squeeze()\n",
    "\n",
    "# Get the true label from the sample (move to GPU if necessary)\n",
    "true_label = sample[\"label\"]\n",
    "\n",
    "# Perform prediction using the all activity strategy\n",
    "all_activity_pred = all_activity(\n",
    "    spikes=spike_record,\n",
    "    assignments=assignments,\n",
    "    n_labels=n_classes\n",
    ")\n",
    "\n",
    "# Perform proportion-based prediction\n",
    "proportion_pred = proportion_weighting(\n",
    "    spikes=spike_record,\n",
    "    assignments=assignments,\n",
    "    proportions=proportions,\n",
    "    n_labels=n_classes\n",
    ")\n",
    "\n",
    "# Reset state variables\n",
    "network.reset_state_variables()\n",
    "\n",
    "# Visualize the input image and print predictions\n",
    "plt.imshow(sample[\"image\"].view(28, 28).cpu(), cmap=\"gray\")\n",
    "plt.title(f\"True Label: {true_label}\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Predicted Label (All Activity): {all_activity_pred.item()}\")\n",
    "print(f\"Predicted Label (Proportion Weighting): {proportion_pred.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n",
      "\n",
      "Begin testing\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 62/10000 [00:40<1:47:46,  1.54it/s]\n",
      "  0%|          | 10/10000 [00:04<1:20:08,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All Activity Accuracy: 0.01%\n",
      "Proportion Weighting Accuracy: 0.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from bindsnet.models import DiehlAndCook2015\n",
    "from bindsnet.datasets import MNIST\n",
    "from bindsnet.encoding import PoissonEncoder\n",
    "from torchvision import transforms\n",
    "from bindsnet.evaluation import all_activity, proportion_weighting\n",
    "from bindsnet.network.monitors import Monitor\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Parameters (same as during training)\n",
    "n_neurons = 100  # Replace with the actual number of neurons used in training\n",
    "time = 250       # Replace with the actual time used during training\n",
    "dt = 1.0         # Replace with the actual time step\n",
    "intensity = 128  # Replace with the actual intensity\n",
    "n_classes = 10   # Number of classes (digits 0-9)\n",
    "\n",
    "# Set the device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on: {device}\")\n",
    "\n",
    "# Initialize the network architecture (same as during training)\n",
    "network = DiehlAndCook2015(\n",
    "    n_inpt=784,\n",
    "    n_neurons=n_neurons,\n",
    "    exc=22.5,    # Replace with actual excitatory current\n",
    "    inh=120,     # Replace with actual inhibitory current\n",
    "    dt=dt,\n",
    "    norm=78.4,   # Replace with actual normalization\n",
    "    theta_plus=0.05,  # Replace with actual threshold increment\n",
    "    inpt_shape=(1, 28, 28),  # Input shape for MNIST (channels, height, width)\n",
    ").to(device)\n",
    "\n",
    "# Load the saved weights, assignments, proportions, and rates\n",
    "checkpoint = torch.load('./snn_train.pt')\n",
    "\n",
    "network.load_state_dict(checkpoint['model_state_dict'])\n",
    "assignments = checkpoint['assignments'].to(device)\n",
    "proportions = checkpoint['proportions'].to(device)\n",
    "rates = checkpoint['rates'].to(device)\n",
    "\n",
    "# Set the network in evaluation mode\n",
    "network.train(mode=False)\n",
    "\n",
    "# Add a spike monitor to the 'Ae' layer\n",
    "spike_monitor = Monitor(network.layers[\"Ae\"], state_vars=[\"s\"], time=int(time / dt), device=device)\n",
    "network.add_monitor(spike_monitor, name=\"Ae_spikes\")\n",
    "\n",
    "# Load the MNIST test dataset\n",
    "test_dataset = MNIST(\n",
    "    PoissonEncoder(time=time, dt=dt),\n",
    "    None,\n",
    "    root=os.path.join(\"..\", \"..\", \"data\", \"MNIST\"),\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Lambda(lambda x: x * intensity)]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Initialize counters for correct predictions\n",
    "correct_all_activity = 0\n",
    "correct_proportion = 0\n",
    "total_samples = len(test_loader)\n",
    "\n",
    "# Run inference on the entire test set\n",
    "print(\"\\nBegin testing\\n\")\n",
    "pbar = tqdm(total=total_samples)\n",
    "\n",
    "for i, sample in enumerate(test_loader):\n",
    "    if i ==10:\n",
    "        break\n",
    "    # Prepare input for the network (move it to GPU if available)\n",
    "    inputs = {\"X\": sample[\"encoded_image\"].view(int(time / dt), 1, 1, 28, 28).to(device)}\n",
    "    \n",
    "    # Run the network on the input\n",
    "    network.run(inputs=inputs, time=time)\n",
    "    \n",
    "    # Record spikes from the excitatory layer\n",
    "    spike_record = torch.zeros((1, int(time / dt), n_neurons), device=device)\n",
    "    spike_record[0] = network.monitors[\"Ae_spikes\"].get(\"s\").squeeze()\n",
    "\n",
    "    # Get the true label from the sample (move to GPU if necessary)\n",
    "    true_label = sample[\"label\"].to(device)\n",
    "\n",
    "    # Perform prediction using the all activity strategy\n",
    "    all_activity_pred = all_activity(\n",
    "        spikes=spike_record,\n",
    "        assignments=assignments,\n",
    "        n_labels=n_classes\n",
    "    )\n",
    "\n",
    "    # Perform proportion-based prediction\n",
    "    proportion_pred = proportion_weighting(\n",
    "        spikes=spike_record,\n",
    "        assignments=assignments,\n",
    "        proportions=proportions,\n",
    "        n_labels=n_classes\n",
    "    )\n",
    "\n",
    "    # Check if the predictions are correct\n",
    "    correct_all_activity += (all_activity_pred == true_label).float().sum().item()\n",
    "    correct_proportion += (proportion_pred == true_label).float().sum().item()\n",
    "\n",
    "    # Reset state variables for the next sample\n",
    "    network.reset_state_variables()\n",
    "    \n",
    "    pbar.update()\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_all_activity = 100.0 * correct_all_activity / total_samples\n",
    "accuracy_proportion = 100.0 * correct_proportion / total_samples\n",
    "\n",
    "print(f\"\\nAll Activity Accuracy: {accuracy_all_activity:.2f}%\")\n",
    "print(f\"Proportion Weighting Accuracy: {accuracy_proportion:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
